{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d6658bf",
   "metadata": {},
   "source": [
    "# 텍스트 사전 준비 작업 - 텍스트 정규화"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83aae765",
   "metadata": {},
   "source": [
    "## 토큰화"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9179278e",
   "metadata": {},
   "source": [
    "### 문장 토큰화\n",
    "- 문장의 마침표, 개행 문자 등 문장의 마지막을 뜻하는 기호에 따라 분리하는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58781540",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "text_sample = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
    "               You can see it out your window or on your television. \\\n",
    "               You feel it when you go to work, or go to church or pay your taxes.'\n",
    "sentences = sent_tokenize(text_sample)\n",
    "print(type(sentences), len(sentences))\n",
    "print(sentences)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d42bc9c",
   "metadata": {},
   "source": [
    "### 단어 토큰화\n",
    "- 문장을 단어로 토큰화하는 것\n",
    "- BoW(Bag of Word)와 같이 단어의 순서가 중요하지 않은 경우 문장 토큰화를 사용하지 않고 단어 토큰화만 사용해도 된다.\n",
    "- 일반적으로 문장 토큰화는 각 문장이 가지는 시맨틱적인 의미가 중요한 요소로 사용될 때 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d6acefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 15\n",
      "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = 'The Matrix is everywhere its all around us, here even in this room.'\n",
    "words = word_tokenize(sentence)\n",
    "print(type(words), len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc4baa6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "# 여러 문장들에 대해 문장별 단어 토큰화 수행\n",
    "def tokenize_text(text):\n",
    "    \n",
    "    sentences = sent_tokenize(text)\n",
    "    word_tokens = [ word_tokenize(sentence) for sentence in sentences ]\n",
    "    return word_tokens\n",
    "\n",
    "word_tokens = tokenize_text(text_sample)\n",
    "print(word_tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44df6789",
   "metadata": {},
   "source": [
    "## stopwords 제거\n",
    "- 분석에 큰 의미가 없는 단어를 제거하는 것을 의미\n",
    "- is, the, a, will 등 문맥적으로 큰 의미가 없는 단어가 이에 속한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64d5c29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 불용어 사전 다운로드\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75a41504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 stop word 갯수: 179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    }
   ],
   "source": [
    "# nltk.corpus.stopwords.words('english') 언어 적기 -- 한국어 지원 안됨\n",
    "print('영어 stop word 갯수:', len(nltk.corpus.stopwords.words('english')))\n",
    "print(nltk.corpus.stopwords.words('english')[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5081067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 단어:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'word_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m all_tokens \u001b[39m=\u001b[39m []\n\u001b[0;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m원본 단어:\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[39mprint\u001b[39m(word_tokens)\n\u001b[0;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m word_tokens:\n\u001b[0;32m      9\u001b[0m     filtered_word \u001b[39m=\u001b[39m []\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "# 위에서 만든 단어 토큰에 불용어 제거\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "all_tokens = []\n",
    "\n",
    "print('원본 단어:')\n",
    "print(word_tokens)\n",
    "\n",
    "for sentence in word_tokens:\n",
    "    filtered_word = []\n",
    "    for word in sentence:\n",
    "        word = word.lower()\n",
    "        if word not in stopwords:\n",
    "            filtered_word.append(word)\n",
    "        else:\n",
    "            continue\n",
    "    all_tokens.append(filtered_word)\n",
    "\n",
    "print()\n",
    "print('불용어 제거 단어')\n",
    "print(all_tokens)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14132211",
   "metadata": {},
   "source": [
    "## 어간추출(Stemming)과 표제어 추출(Lemmatization)\n",
    "- 문법적 또는 의미적으로 변화하는 **단어의 원형을 찾는다.**\n",
    "- ex) worked, works, working의 동사 원형인 work를 찾는다.\n",
    "- Lemmatization이 Stemming보다 정교하며 의미론적인 기반에서 단어의 원형을 찾는다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96bfb0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amus amus amus\n"
     ]
    }
   ],
   "source": [
    "# 어간추출\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "# work\n",
    "print(stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked'))\n",
    "# amuse\n",
    "print(stemmer.stem('amusing'), stemmer.stem('amuses'), stemmer.stem('amused'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "48b4c0e7",
   "metadata": {},
   "source": [
    "## DTM (문서 단어 행렬)\n",
    "- 다수의 문서에서 등장하는 각 단어들의 빈도를 행렬로 표현한 것\n",
    "- 사이킷런의 CounterVectorizer를 사용하여 생성할 수 있다.\n",
    "- CounterVectorizer를 이용한 피처 벡터화\n",
    "    - 모든 문자를 소문자로 변환 -> 토큰화 -> 텍스트 정규화 stop Words 필터링만 수행 -> 토큰 된 단어들을 추출하여 vectorization 적용"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2cbca920",
   "metadata": {},
   "source": [
    "- Countervectorizer 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1d0bede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words vector:  [[1 1 2 1 2 1]]\n",
      "vocabulary:  {'you': 4, 'know': 1, 'want': 3, 'your': 5, 'love': 2, 'because': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['you know I want your love. because I Love you.']\n",
    "vector = CountVectorizer()\n",
    "\n",
    "# 문서로부터 각 단어의 빈도수 산출\n",
    "print('bag of words vector: ', vector.fit_transform(corpus).toarray())\n",
    "# I는 BoW를 만드는 과정에서 사라졌는대, 이는 countervectroizer가 기본적으로 길이가 2이상인 문자에 대해서만 토큰으로 인식 하기 때문이다\n",
    "print('vocabulary: ',vector.vocabulary_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "505ac2a5",
   "metadata": {},
   "source": [
    "### 불용어를 제거한 BoW 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86004468",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23db3b90",
   "metadata": {},
   "source": [
    "(1) 사용자 정의 불용어 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46ded2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words vector:  [[1 1 1 1 1 1]]\n",
      "vocabulary: {'family': 2, 'an': 0, 'important': 3, 'thing': 5, 'it': 4, 'everything': 1}\n"
     ]
    }
   ],
   "source": [
    "text = [\"Family is not an important thing. It's everything.\"]\n",
    "vect = CountVectorizer(stop_words=['the','en','a','is','not'])\n",
    "print('bag of words vector: ' , vect.fit_transform(text).toarray())\n",
    "print('vocabulary:',vect.vocabulary_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14d64ea3",
   "metadata": {},
   "source": [
    "(2) CountVectorizer에서 제공하는 잧체 불용어 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dc472d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words vector:  [[1 1 1]]\n",
      "vocabulary: {'family': 0, 'important': 1, 'thing': 2}\n"
     ]
    }
   ],
   "source": [
    "text = [\"Family is not an important thing. It's everything.\"]\n",
    "vect = CountVectorizer(stop_words='english')\n",
    "print('bag of words vector: ' , vect.fit_transform(text).toarray())\n",
    "print('vocabulary:',vect.vocabulary_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3449ad7",
   "metadata": {},
   "source": [
    "(3) ntlk에서 제공하는 불용어 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "053661d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words vector:  [[1 1 1 1]]\n",
      "vocabulary: {'family': 1, 'important': 2, 'thing': 3, 'everything': 0}\n"
     ]
    }
   ],
   "source": [
    "text = [\"Family is not an important thing. It's everything.\"]\n",
    "stop_words = stopwords.words('english')\n",
    "vect = CountVectorizer(stop_words=stop_words)\n",
    "print('bag of words vector: ' , vect.fit_transform(text).toarray())\n",
    "print('vocabulary:',vect.vocabulary_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1054732c",
   "metadata": {},
   "source": [
    "## TF-IDF(Term Frequency-Inverse Doucment Frequency)\n",
    "- TfidfVectorizer 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2dcb898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46735098 0.         0.46735098 0.         0.46735098\n",
      "  0.         0.35543247 0.46735098]\n",
      " [0.         0.         0.79596054 0.         0.         0.\n",
      "  0.         0.60534851 0.        ]\n",
      " [0.57735027 0.         0.         0.         0.57735027 0.\n",
      "  0.57735027 0.         0.        ]]\n",
      "[('do', 0), ('know', 1), ('like', 2), ('love', 3), ('should', 4), ('want', 5), ('what', 6), ('you', 7), ('your', 8)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = ['you know I want your love', # 문서1\n",
    "          'I like you', # 문서2\n",
    "          'what should I do' # 문서3\n",
    "        ]\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "print(tfidf.fit_transform(corpus).toarray())\n",
    "print(sorted(tfidf.vocabulary_.items(), key = lambda x:x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23005ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe88bfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e739ac6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d914cd05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "248.225px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
